{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert HEALPix Zarr data to netCDF with variable subsets for TempestExtremes<br>\n",
    "<br>\n",
    "## This code compute the uivt and vivt based on ua, va, and hus<br>\n",
    "<br>\n",
    "## Author:<br>\n",
    "- Zhe Feng || zhe.feng@pnnl.gov\n",
    "- Ziming Chen || ziming.chen@pnnl.gov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easygems.healpix as egh\n",
    "import intake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[2]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(intake.open_catalog(\"https://digital-earths-global-hackathon.github.io/catalog/catalog.yaml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[3]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the NERSC catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_location = \"NERSC\" # \"online\" # \n",
    "cat = intake.open_catalog(\"https://digital-earths-global-hackathon.github.io/catalog/catalog.yaml\")[current_location]\n",
    "list(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[4]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_Model = \"nicam_gl11\" # \"icon_d3hp003\" # \"um_glm_n2560_RAL3p3\" # \"casesm2_10km_nocumulus\" # \"icon_ngc4008\" # \n",
    "s_TimeRes = \"PT6H\"\n",
    "#\n",
    "zoom = 8\n",
    "pd.DataFrame(cat[s_Model].describe()[\"user_parameters\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data into a Data Set<br>\n",
    "most datasets have a `zoom` parameter. We will use `zoom` level 8 [(~24km)](https://easy.gems.dkrz.de/Processing/healpix/index.html#healpix-spatial-resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[5]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds0  = cat[s_Model](zoom=zoom, time=s_TimeRes).to_dask()\n",
    "ds0  = ds0.pipe(egh.attach_coords)\n",
    "ds0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables to output<br>\n",
    "# RawVarName: output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varout_dict       = { 'time' : 'time',\n",
    "                      'lat' : 'lat',\n",
    "                      'lon' : 'lon',\n",
    "                      'pressure': 'lev',\n",
    "                      'ua': 'ua',\n",
    "                      'va': 'va',\n",
    "                      'hus': 'hus',\n",
    "                      'orog': 'ELEV',\n",
    "                      'pr' : 'pr',\n",
    "                      'prs': 'prs',\n",
    "                      'ps' : 'ps',\n",
    "                      'psl' : 'psl',\n",
    "                      'uas' : 'uas',\n",
    "                      'vas' : 'vas',\n",
    "                      'sfcWind' : 'sfcWind',\n",
    "                      'zg' : 'zg',\n",
    "                      'ta' : 'ta',\n",
    "                      'tas': 'tas',\n",
    "                      'rlut': 'rlut',\n",
    "                     }\n",
    "print(ds0.data_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[22]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset variables and rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varout = []\n",
    "varout_RawVarName = []\n",
    "ds     = {}\n",
    "for var in varout_dict:\n",
    "    if var in ds0.data_vars: #  or var in ds0.coords\n",
    "        ds[varout_dict[var]] = ds0[var]\n",
    "        #\n",
    "    else:\n",
    "        print(f\"No {var}: {varout_dict[var]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds    = xr.Dataset(ds)\n",
    "print(ds)\n",
    "ds.attrs = ds0.attrs\n",
    "for var in ds0.coords:\n",
    "    if var in varout_dict and var != varout_dict[var]:\n",
    "        ds    = ds.rename_dims({var: varout_dict[var]})\n",
    "        ds    = ds.rename_vars({var: varout_dict[var]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.data_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " === Ziming Chen 05/11/12 ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['time'] = xr.decode_cf(ds).indexes['time'] #.to_datetimeindex()\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the uivt and vivt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python function for vertical mass integration using xarray and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertical_mass_integration(hus: xr.DataArray, ps: xr.DataArray, plev: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Perform vertical integration of specific humidity (hus) in pressure coordinates.\n",
    "    Parameters:\n",
    "    - hus: xr.DataArray with dimensions (time, lev, cell)\n",
    "    - ps: xr.DataArray with dimensions (time, cell), surface pressure in hPa\n",
    "    - plev: xr.DataArray with dimension (lev), pressure levels in hPa\n",
    "    Returns:\n",
    "    - xr.DataArray with dimensions (time, cell) representing vertically integrated hus\n",
    "    \"\"\"\n",
    "    # Ensure pressure levels are sorted from top (min) to bottom (max)\n",
    "    if not np.all(np.diff(plev.values) > 0):\n",
    "        hus = hus.sel(lev=plev[::-1])\n",
    "        plev = plev[::-1]\n",
    "    #\n",
    "    if ps.max() < 1200: # hPa to Pa for ps\n",
    "        ps                    = ps * 100\n",
    "        ps.name               = \"Pa\"\n",
    "        ps.attrs[\"units\"]     = \"Pa\"\n",
    "        ps.attrs[\"long_name\"] = \"Estimated surface pressure\"\n",
    "    # Mask hus where pressure level > surface pressure\n",
    "    plev_3d = plev * xr.ones_like(hus)\n",
    "    ps_3d = ps * xr.ones_like(hus)\n",
    "    hus_masked = hus.where(plev_3d <= ps_3d)\n",
    "\n",
    "    # Integrate using trapezoidal rule in pressure coordinates (in Pa)\n",
    "    dp = np.gradient(plev.values) * 100.0  # convert hPa to Pa\n",
    "    dp = xr.DataArray(dp, coords={\"lev\": plev}, dims=[\"lev\"])\n",
    "    dp_3d = dp * xr.ones_like(hus)\n",
    "    g = 9.8  # gravity\n",
    "    integrand = hus_masked * dp_3d / g\n",
    "    result = integrand.sum(dim=\"lev\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = f'/pscratch/sd/w/wcmca1/scream-cess-healpix/data4TE/{s_Model}_{s_TimeRes}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: create output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up attributes before writing to NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_attrs_for_netcdf(ds):\n",
    "    # Make a copy to avoid modifying the original\n",
    "    attrs = ds.attrs.copy()\n",
    "    \n",
    "    # Handle dictionary and boolean attributes\n",
    "    for key, value in list(attrs.items()):\n",
    "        if isinstance(value, dict):\n",
    "            # Remove dictionary attributes\n",
    "            ds.attrs.pop(key, None)\n",
    "        elif isinstance(value, bool):\n",
    "            # Convert boolean to integer (1 for True, 0 for False)\n",
    "            ds.attrs[key] = int(value)\n",
    "    \n",
    "    # Check all variables too\n",
    "    if hasattr(ds, 'variables'):\n",
    "        for var in ds.variables:\n",
    "            var_attrs = ds[var].attrs.copy()\n",
    "            for key, value in list(var_attrs.items()):\n",
    "                if isinstance(value, dict):\n",
    "                    import json\n",
    "                    ds[var].attrs[key] = json.dumps(value)\n",
    "                elif isinstance(value, bool):\n",
    "                    # Convert boolean to integer\n",
    "                    ds[var].attrs[key] = int(value)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by month and write each to a separate file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
